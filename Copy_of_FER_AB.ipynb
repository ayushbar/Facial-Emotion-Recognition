{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of FER AB",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqMM73DWZ98U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Law8UGxTZ5OA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f54620b-a157-4340-93dd-57d2ac4a47f4"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = \"CK+48.zip\"\n",
        "\n",
        "with ZipFile(file_name,'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('done')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzp5TDJWO2Qh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d71b8878-c10c-47d5-a93a-f80f98a98825"
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3C3VhmqZBAy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "d88905d1-e327-4055-c11c-2d96e17e5d22"
      },
      "source": [
        "num_classes = 7\n",
        "img_rows, img_cols = 48, 48\n",
        "batch_size = 32\n",
        "\n",
        "train_data_dir = '/content/CK+48/train'\n",
        "validation_data_dir = '/content/CK+48/validation'\n",
        "\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=30,\n",
        "      shear_range=0.3,\n",
        "      zoom_range=0.5,\n",
        "      width_shift_range=0.4,\n",
        "      height_shift_range=0.4,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        " \n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        color_mode = 'grayscale',\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        color_mode = 'grayscale',\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 821 images belonging to 7 classes.\n",
            "Found 207 images belonging to 7 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py:179: UserWarning: Using \".tiff\" files with multiple bands will cause distortion. Please verify your output.\n",
            "  warnings.warn('Using \".tiff\" files with multiple bands '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umPLD3vlpM8c"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.layers.advanced_activations import ELU\n",
        "from keras.layers.core import Activation, Flatten, Dropout, Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHk74xr6pz5w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ad874210-9c10-45cc-cb14-35329e15385b"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",\n",
        "                 input_shape = (img_rows, img_cols, 1)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, (3, 3), padding = \"same\", kernel_initializer=\"he_normal\", \n",
        "                 input_shape = (img_rows, img_cols, 1)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Block #2: second CONV => RELU => CONV => RELU => POOL\n",
        "# layer set\n",
        "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Block #3: third CONV => RELU => CONV => RELU => POOL\n",
        "# layer set\n",
        "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Block #4: third CONV => RELU => CONV => RELU => POOL\n",
        "# layer set\n",
        "model.add(Conv2D(512, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(512, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Block #5: first set of FC => RELU layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Block #6: second set of FC => RELU layers\n",
        "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Block #7: softmax classifier\n",
        "model.add(Dense(num_classes, kernel_initializer=\"he_normal\"))\n",
        "model.add(Activation(\"softmax\"))\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 48, 48, 32)        320       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 48, 48, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 48, 48, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 48, 48, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 48, 48, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 24, 24, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 12, 12, 128)       73856     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 12, 12, 128)       147584    \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 6, 6, 256)         295168    \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 6, 6, 256)         590080    \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 3, 3, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 3, 3, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                32832     \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 7)                 455       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 7)                 0         \n",
            "=================================================================\n",
            "Total params: 4,757,543\n",
            "Trainable params: 4,753,319\n",
            "Non-trainable params: 4,224\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZo2n0KDqCjE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b52f599-7f06-4833-d2bd-7d48f0b9b138"
      },
      "source": [
        "from keras.optimizers import RMSprop, SGD, Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('val_accuracy')>0.93):\n",
        "      print(\"93% hence cancelling\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "callbacks = myCallback()\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = Adam(lr=0.001),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "nb_train_samples = 821\n",
        "nb_validation_samples = 207\n",
        "epochs = 300\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch = nb_train_samples // batch_size,\n",
        "    epochs = epochs,\n",
        "    callbacks = [callbacks],\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps = nb_validation_samples // batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "25/25 [==============================] - 33s 1s/step - loss: 0.1578 - accuracy: 0.9480 - val_loss: 0.6976 - val_accuracy: 0.8229\n",
            "Epoch 2/300\n",
            "25/25 [==============================] - 30s 1s/step - loss: 0.1535 - accuracy: 0.9518 - val_loss: 0.4395 - val_accuracy: 0.8286\n",
            "Epoch 3/300\n",
            "25/25 [==============================] - 31s 1s/step - loss: 0.1314 - accuracy: 0.9563 - val_loss: 0.1728 - val_accuracy: 0.8629\n",
            "Epoch 4/300\n",
            "25/25 [==============================] - 30s 1s/step - loss: 0.1416 - accuracy: 0.9589 - val_loss: 1.4045 - val_accuracy: 0.7371\n",
            "Epoch 5/300\n",
            "25/25 [==============================] - 31s 1s/step - loss: 0.1883 - accuracy: 0.9450 - val_loss: 0.2119 - val_accuracy: 0.7657\n",
            "Epoch 6/300\n",
            "25/25 [==============================] - 31s 1s/step - loss: 0.1847 - accuracy: 0.9430 - val_loss: 2.8082 - val_accuracy: 0.7429\n",
            "Epoch 7/300\n",
            "25/25 [==============================] - 30s 1s/step - loss: 0.1963 - accuracy: 0.9383 - val_loss: 0.0068 - val_accuracy: 0.8343\n",
            "Epoch 8/300\n",
            "25/25 [==============================] - 31s 1s/step - loss: 0.1611 - accuracy: 0.9455 - val_loss: 0.6817 - val_accuracy: 0.8490\n",
            "Epoch 9/300\n",
            "25/25 [==============================] - 30s 1s/step - loss: 0.1651 - accuracy: 0.9582 - val_loss: 1.0080 - val_accuracy: 0.7886\n",
            "Epoch 10/300\n",
            "25/25 [==============================] - 30s 1s/step - loss: 0.1296 - accuracy: 0.9582 - val_loss: 0.3394 - val_accuracy: 0.8629\n",
            "Epoch 11/300\n",
            "25/25 [==============================] - 31s 1s/step - loss: 0.1884 - accuracy: 0.9425 - val_loss: 1.1223 - val_accuracy: 0.8629\n",
            "Epoch 12/300\n",
            "25/25 [==============================] - 30s 1s/step - loss: 0.1537 - accuracy: 0.9493 - val_loss: 0.1413 - val_accuracy: 0.8000\n",
            "Epoch 13/300\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.1265 - accuracy: 0.9556 - val_loss: 2.1703 - val_accuracy: 0.6743\n",
            "Epoch 14/300\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.1269 - accuracy: 0.9653 - val_loss: 0.0022 - val_accuracy: 0.7543\n",
            "Epoch 15/300\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.1568 - accuracy: 0.9455 - val_loss: 0.9107 - val_accuracy: 0.8281\n",
            "Epoch 16/300\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.1612 - accuracy: 0.9531 - val_loss: 0.9915 - val_accuracy: 0.7771\n",
            "Epoch 17/300\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.1945 - accuracy: 0.9400 - val_loss: 0.3134 - val_accuracy: 0.8000\n",
            "Epoch 18/300\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.1732 - accuracy: 0.9455 - val_loss: 2.6823 - val_accuracy: 0.7143\n",
            "Epoch 19/300\n",
            "25/25 [==============================] - 31s 1s/step - loss: 0.1750 - accuracy: 0.9460 - val_loss: 0.2371 - val_accuracy: 0.8114\n",
            "Epoch 20/300\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.2011 - accuracy: 0.9362 - val_loss: 3.7756 - val_accuracy: 0.6971\n",
            "Epoch 21/300\n",
            "25/25 [==============================] - 31s 1s/step - loss: 0.2090 - accuracy: 0.9366 - val_loss: 0.0032 - val_accuracy: 0.8686\n",
            "Epoch 22/300\n",
            "25/25 [==============================] - 31s 1s/step - loss: 0.1674 - accuracy: 0.9392 - val_loss: 0.5108 - val_accuracy: 0.8594\n",
            "Epoch 23/300\n",
            "25/25 [==============================] - 31s 1s/step - loss: 0.1733 - accuracy: 0.9404 - val_loss: 0.2937 - val_accuracy: 0.8286\n",
            "Epoch 24/300\n",
            "25/25 [==============================] - 31s 1s/step - loss: 0.1314 - accuracy: 0.9594 - val_loss: 0.0297 - val_accuracy: 0.8057\n",
            "Epoch 25/300\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.1414 - accuracy: 0.9569 - val_loss: 0.8528 - val_accuracy: 0.8000\n",
            "Epoch 26/300\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.1011 - accuracy: 0.9670 - val_loss: 0.0781 - val_accuracy: 0.7943\n",
            "Epoch 27/300\n",
            "25/25 [==============================] - 31s 1s/step - loss: 0.1405 - accuracy: 0.9417 - val_loss: 2.7576 - val_accuracy: 0.7943\n",
            "Epoch 28/300\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.1843 - accuracy: 0.9455 - val_loss: 0.0162 - val_accuracy: 0.8971\n",
            "Epoch 29/300\n",
            "25/25 [==============================] - 31s 1s/step - loss: 0.1318 - accuracy: 0.9518 - val_loss: 0.7552 - val_accuracy: 0.8594\n",
            "Epoch 30/300\n",
            "25/25 [==============================] - 31s 1s/step - loss: 0.1516 - accuracy: 0.9544 - val_loss: 0.1883 - val_accuracy: 0.8800\n",
            "Epoch 31/300\n",
            "25/25 [==============================] - 31s 1s/step - loss: 0.1588 - accuracy: 0.9569 - val_loss: 0.2417 - val_accuracy: 0.8457\n",
            "Epoch 32/300\n",
            "25/25 [==============================] - 31s 1s/step - loss: 0.1622 - accuracy: 0.9379 - val_loss: 1.9137 - val_accuracy: 0.7714\n",
            "Epoch 33/300\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.1719 - accuracy: 0.9468 - val_loss: 0.0476 - val_accuracy: 0.7886\n",
            "Epoch 34/300\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.1875 - accuracy: 0.9392 - val_loss: 2.1637 - val_accuracy: 0.8000\n",
            "Epoch 35/300\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.1506 - accuracy: 0.9480 - val_loss: 0.5852 - val_accuracy: 0.8914\n",
            "Epoch 36/300\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.1421 - accuracy: 0.9538 - val_loss: 0.4570 - val_accuracy: 0.8958\n",
            "Epoch 37/300\n",
            "25/25 [==============================] - 31s 1s/step - loss: 0.1816 - accuracy: 0.9434 - val_loss: 0.4176 - val_accuracy: 0.8971\n",
            "Epoch 38/300\n",
            "25/25 [==============================] - 31s 1s/step - loss: 0.1464 - accuracy: 0.9455 - val_loss: 0.0240 - val_accuracy: 0.8629\n",
            "Epoch 39/300\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.1177 - accuracy: 0.9600 - val_loss: 0.7341 - val_accuracy: 0.8629\n",
            "Epoch 40/300\n",
            "25/25 [==============================] - 31s 1s/step - loss: 0.1170 - accuracy: 0.9582 - val_loss: 0.0182 - val_accuracy: 0.8686\n",
            "Epoch 41/300\n",
            "25/25 [==============================] - 31s 1s/step - loss: 0.1278 - accuracy: 0.9602 - val_loss: 1.1110 - val_accuracy: 0.9029\n",
            "Epoch 42/300\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.1144 - accuracy: 0.9663 - val_loss: 0.0011 - val_accuracy: 0.9143\n",
            "Epoch 43/300\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.1400 - accuracy: 0.9569 - val_loss: 0.4074 - val_accuracy: 0.8854\n",
            "Epoch 44/300\n",
            "25/25 [==============================] - 31s 1s/step - loss: 0.1312 - accuracy: 0.9480 - val_loss: 0.1909 - val_accuracy: 0.8914\n",
            "Epoch 45/300\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.1031 - accuracy: 0.9696 - val_loss: 0.6497 - val_accuracy: 0.8800\n",
            "Epoch 46/300\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.1288 - accuracy: 0.9569 - val_loss: 1.3646 - val_accuracy: 0.8457\n",
            "Epoch 47/300\n",
            "25/25 [==============================] - 33s 1s/step - loss: 0.1418 - accuracy: 0.9544 - val_loss: 0.1522 - val_accuracy: 0.8571\n",
            "Epoch 48/300\n",
            "25/25 [==============================] - 32s 1s/step - loss: 0.1562 - accuracy: 0.9499 - val_loss: 1.6173 - val_accuracy: 0.8514\n",
            "Epoch 49/300\n",
            "25/25 [==============================] - 33s 1s/step - loss: 0.1092 - accuracy: 0.9607 - val_loss: 0.0042 - val_accuracy: 0.9371\n",
            "93% hence cancelling\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxPR-_WRTtOm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "c8bb5a3b-321a-4dba-c443-814df45fe0db"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbXbsg4VWJD1"
      },
      "source": [
        "model.save(\"/content/drive/My Drive/model_cnn/{NAME}.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO9_SxIdW-7o"
      },
      "source": [
        "model.save(\"/content/drive/My Drive/model_fer.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGQXxXtBLWzf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "outputId": "806e6c63-a78f-44c9-b102-619c8d80d8a7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "nb_train_samples = 821\n",
        "nb_validation_samples = 207\n",
        "\n",
        "# We need to recreate our validation generator with shuffle = false\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        color_mode = 'grayscale',\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)\n",
        "\n",
        "class_labels = validation_generator.class_indices\n",
        "class_labels = {v: k for k, v in class_labels.items()}\n",
        "classes = list(class_labels.values())\n",
        "\n",
        "#Confution Matrix and Classification Report\n",
        "Y_pred = model.predict_generator(validation_generator, nb_validation_samples // batch_size+1)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(validation_generator.classes, y_pred))\n",
        "print('Classification Report')\n",
        "target_names = list(class_labels.values())\n",
        "print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "cnf_matrix = confusion_matrix(validation_generator.classes, y_pred)\n",
        "\n",
        "plt.imshow(cnf_matrix, interpolation='nearest')\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(classes))\n",
        "_ = plt.xticks(tick_marks, classes, rotation=90)\n",
        "_ = plt.yticks(tick_marks, classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 207 images belonging to 7 classes.\n",
            "Confusion Matrix\n",
            "[[15  0  0  0  9  3  0]\n",
            " [ 0 36  0  0  0  0  0]\n",
            " [ 0  0 10  0  5  0  0]\n",
            " [ 0  0  0 42  0  0  0]\n",
            " [ 0  0  0  0 20  0  0]\n",
            " [ 0  0  0  0  3 14  0]\n",
            " [ 3  0  0  0  0  0 47]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       angry       0.83      0.56      0.67        27\n",
            "     disgust       1.00      1.00      1.00        36\n",
            "        fear       1.00      0.67      0.80        15\n",
            "       happy       1.00      1.00      1.00        42\n",
            "     neutral       0.54      1.00      0.70        20\n",
            "         sad       0.82      0.82      0.82        17\n",
            "    surprise       1.00      0.94      0.97        50\n",
            "\n",
            "    accuracy                           0.89       207\n",
            "   macro avg       0.89      0.86      0.85       207\n",
            "weighted avg       0.92      0.89      0.89       207\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIXFRNQIkdXu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "5b115525-48c5-41a4-e0c8-009696d9ed07"
      },
      "source": [
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        color_mode = 'grayscale',\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)\n",
        "\n",
        "class_labels = validation_generator.class_indices\n",
        "class_labels = {v: k for k, v in class_labels.items()}\n",
        "classes = list(class_labels.values())\n",
        "print(class_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 207 images belonging to 7 classes.\n",
            "{0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'surprise'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtoSHVweOTAX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "998a9ad9-0d0d-4731-ac3b-8a644635d441"
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "\n",
        "    ret, frame = cap.read()\n",
        "    cv2_imshow(frame)\n",
        "    if cv2.waitKey(1) == 9: #9 is the Enter Key\n",
        "        break\n",
        "        \n",
        "cap.release()\n",
        "cv2.destroyAllWindows()     "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-bd4d4b20e28c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#9 is the Enter Key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/patches/__init__.py\u001b[0m in \u001b[0;36mcv2_imshow\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \"\"\"\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0;31m# cv2 stores colors as BGR; convert to RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'clip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCqanMJJuK6Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "aa88d9f9-638e-4d1b-aafc-c5e9695b80d3"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from time import sleep\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "face_classifier = cv2.CascadeClassifier('/content/CK+48/Haarcascades/haarcascade_frontalface_default.xml')\n",
        "\n",
        "def face_detector(img, che):\n",
        "    # Convert image to grayscale\n",
        "      # if ret is True:\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
        "    if faces is ():\n",
        "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
        "    \n",
        "    for (x,y,w,h) in faces:\n",
        "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "        roi_gray = gray[y:y+h, x:x+w]\n",
        "\n",
        "    try:\n",
        "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_AREA)\n",
        "    except:\n",
        "        return (x,w,y,h), np.zeros((48,48), np.uint8), img\n",
        "    return (x,w,y,h), roi_gray, img\n",
        "        \n",
        "    # ... other code ...\n",
        "  \n",
        "\n",
        "    \n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "\n",
        "    ret, frame = cap.read()\n",
        "    rect, face, image = face_detector(frame, ret)\n",
        "    if np.sum([face]) != 0.0:\n",
        "        roi = face.astype(\"float\") / 255.0\n",
        "        roi = img_to_array(roi)\n",
        "        roi = np.expand_dims(roi, axis=0)\n",
        "\n",
        "        # make a prediction on the ROI, then lookup the class\n",
        "        preds = str(classifier.predict_classes(roi, 1, verbose = 0)[0])\n",
        "\n",
        "        if preds == \"[0]\":\n",
        "            label = \"Angry\"\n",
        "        if preds == \"[1]\":\n",
        "            label = \"Disgust\" \n",
        "        if preds == \"[2]\":\n",
        "            label = \"Fear\"       \n",
        "        if preds == \"[3]\":\n",
        "            label = \"Happy\"\n",
        "        if preds == \"[4]\":\n",
        "            label = \"Neutral\"\n",
        "        if preds == \"[5]\":\n",
        "            label = \"Sad\"\n",
        "        if preds == \"[6]\":\n",
        "            label = \"Surprise\"                \n",
        "\n",
        "        label_position = (rect[0] + int((rect[1]/2)), rect[2] + 25)\n",
        "        cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
        "    else:\n",
        "        cv2.putText(image, \"No Face Found\", (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
        "        \n",
        "    cv2.imshow('All', image)\n",
        "    if cv2.waitKey(1) == 9: #9 is the Enter Key\n",
        "        break\n",
        "        \n",
        "cap.release()\n",
        "cv2.destroyAllWindows()     "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-e25e5d8c6f85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mroi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"float\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-e25e5d8c6f85>\u001b[0m in \u001b[0;36mface_detector\u001b[0;34m(img, che)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Convert image to grayscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0;31m# if ret is True:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfaces\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ]
    }
  ]
}